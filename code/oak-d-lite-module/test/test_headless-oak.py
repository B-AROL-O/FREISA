import depthai as dai
import cv2
import blobconverter
import numpy as np
from datetime import datetime
import time


def frameNorm(frame, bbox):
    normVals = np.full(len(bbox), frame.shape[0])
    normVals[::2] = frame.shape[1]
    return (np.clip(np.array(bbox), 0, 1) * normVals).astype(int)


VERB = True
DISPLAY = True
MN_CLASSES = [
    "background",
    "aeroplane",
    "bicycle",
    "bird",
    "boat",
    "bottle",
    "bus",
    "car",
    "cat",
    "chair",
    "cow",
    "diningtable",
    "dog",
    "horse",
    "motorbike",
    "person",
    "pottedplant",
    "sheep",
    "sofa",
    "train",
    "tvmonitor",
]

# Initialize DepthAI pipeline
pipeline = dai.Pipeline()

# Create camera node
cam_rgb = pipeline.create(dai.node.ColorCamera)
cam_rgb.setPreviewSize(300, 300)
cam_rgb.setInterleaved(False)

# Configure the YOLO detection model (use MobileNet for testing)
# pipeline.add(depthai.YoloDetectionNetwork().setBlobPath("path/to/your/yolo_model.blob").setConfidenceThreshold(0.5))
detection_nn = pipeline.create(dai.node.MobileNetDetectionNetwork)
detection_nn.setBlobPath(blobconverter.from_zoo(name="mobilenet-ssd", shaves=6))
detection_nn.setConfidenceThreshold(0.5)

# Connect color camera preview to nn input
cam_rgb.preview.link(detection_nn.input)

# Create XLink objects and link the specific node outputs
# to the corresponding stream
# Video
xout_rgb = pipeline.create(dai.node.XLinkOut)
xout_rgb.setStreamName("rgb")
cam_rgb.preview.link(xout_rgb.input)

# Inference
xout_nn = pipeline.create(dai.node.XLinkOut)
xout_nn.setStreamName("inference")
detection_nn.out.link(xout_nn.input)

# Create DepthAI device
with dai.Device(pipeline) as device:
    # Define queue for nn output - blocking=False will make only the most recent info available
    # queue_nn = device.getOutputQueue(name="inference", maxSize=1, blocking=False)
    queue_nn = device.getOutputQueue(name="inference")

    # queue_rgb = device.getOutputQueue("rgb", maxSize=1, blocking=False)
    queue_rgb = device.getOutputQueue("rgb")

    # Initialize placeholders for results:
    frame = None  # Probably not used
    detections = []

    # Create a text file to store inference results
    with open("detection_results.txt", "w") as output_file:
        print("Capture started")
        while True:
            # Timestamp
            ts = datetime.now().strftime("%m/%d/%Y, %H:%M:%S")

            # Try to get an element from the output nn queue
            in_nn = queue_nn.tryGet()

            # frame = None
            if DISPLAY:
                in_rgb = queue_rgb.tryGet()

                if in_rgb is not None:
                    frame = in_rgb.getCvFrame()

            flg_disp = False
            if frame is not None and DISPLAY:
                flg_disp = True

            # Get the detection results from the frame (if any)
            if in_nn is not None:
                detections = in_nn.detections

                for detection in detections:
                    # Get boundary coordinates of detected object
                    x1, y1, x2, y2 = (
                        int(detection.xmin),
                        int(detection.ymin),
                        int(detection.xmax),
                        int(detection.ymax),
                    )
                    # Notice that the values of x and y are normalized to [0, 1]
                    object_centroid = (0.5 * (x1 + x2), 0.5 * (y1 + y2))

                    out_str = (
                        "{}: Label: {} - {}, Confidence: {}; Position: {}\n".format(
                            ts,
                            detection.label,
                            MN_CLASSES[detection.label],
                            detection.confidence,
                            object_centroid,
                        )
                    )

                    if VERB:
                        print(out_str)

                    output_file.write(out_str)

                    if flg_disp:
                        bbox = frameNorm(frame, (x1, y1, x2, y2))
                        cv2.rectangle(
                            frame,
                            (bbox[0], bbox[1]),
                            (bbox[2], bbox[3]),
                            (255, 0, 0),
                            2,
                        )

                if flg_disp:
                    print("here")
                    cv2.imshow("preview", frame)
            else:
                pass
                # output_file.write("{}\n".format(ts))

            # wait for some time before next capture
            time.sleep(0.5)

            ###
            # This was generated by Chat GPT
            ###

            # Optionally, you can process the detections further, draw bounding boxes on the frame, etc.
            # for detection in detections:
            #     x1, y1, x2, y2 = int(detection.xmin), int(detection.ymin), int(detection.xmax), int(detection.ymax)
            #     cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)

            # Display the frame (optional)
            # cv2.imshow("YOLO Detection", frame)
            # cv2.waitKey(1)

            # Break the loop when 'q' is pressed
            # if cv2.waitKey(1) == ord('q'):
            #     break
